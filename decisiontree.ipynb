{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree implementation using Entropy as the cost function\n",
    "\n",
    "Mamello Maseko and \n",
    "Sandile Shongwe (1236067)\n",
    "\n",
    "References:\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from random import seed\n",
    "%xmode plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups, classes):\n",
    "    n_instances = float(np.sum([len(i) for i in groups]))\n",
    "    gini_value = 0.0\n",
    "    for grp in groups:\n",
    "        size = float(len(grp))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for val in classes:\n",
    "            proportion = [ i[-1]  for i in grp]\n",
    "            proportion = proportion.count(val)/size\n",
    "            score += proportion * proportion\n",
    "        gini_value += (1.0-score)*(size/n_instances)\n",
    "    return gini_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(entropy_sys, groups, classes):\n",
    "    tot_entropy = 0.0\n",
    "    for grp in groups:\n",
    "        if len(grp)==0:\n",
    "            continue\n",
    "        entropy = 0.0\n",
    "        tmp_arr = (np.array(grp))\n",
    "        prob = tmp_arr[:,-1].sum()/len(grp)\n",
    "        entropy -= prob*np.log2(prob) +(1-prob)*np.log2((1-prob))\n",
    "        tot_entropy += entropy\n",
    "    tot_entropy = tot_entropy/(len(groups[0])+len(groups[1]))\n",
    "    gain = entropy_sys - tot_entropy\n",
    "    return gain\n",
    "\n",
    "def get_split_g(data):\n",
    "\n",
    "    idx_split = 100\n",
    "    value_split = 100\n",
    "    max_gain = -1000\n",
    "    grp1 = None\n",
    "    grp2 = None\n",
    "\n",
    "    sys_prob = (np.array(data)[:,-1].sum())/len(data)\n",
    "    sys_gain = -(sys_prob)*np.log2(sys_prob) - (1-sys_prob)*np.log2((1-sys_prob))\n",
    "    for i in range(len(data[0])-1):\n",
    "        for j in range(len(data)):\n",
    "            tgrp1, tgrp2 = separate(i, data[j][i], data)\n",
    "            tmp = informationGain(sys_prob,[tgrp1, tgrp2], [1,0])\n",
    "            if tmp > max_gain:\n",
    "                max_gain = tmp\n",
    "                idx_split = i\n",
    "                value_split = data[j][i]\n",
    "                grp1 = tgrp1\n",
    "                grp2 = tgrp2\n",
    "    return {'idx':idx_split, 'val':value_split, 'gscore': max_gain, 'groups':[grp1, grp2] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits the data into groups according to value \n",
    "#and the column index idx\n",
    "def separate(idx, value, data):\n",
    "    grp1 = []\n",
    "    grp2 = []\n",
    "    for instance in data:\n",
    "        if instance[idx] < value:\n",
    "            grp1.append(instance)\n",
    "        else:\n",
    "            grp2.append(instance)\n",
    "    return grp1, grp2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_of_split(group):\n",
    "    positive = [row[-1] for row in group].count(1)\n",
    "    negative = [row[-1] for row in group].count(0)\n",
    "    if(positive > negative):\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    groups = node['groups']\n",
    "    left = groups[0]\n",
    "    right = groups[1]\n",
    "    del(node['groups'])\n",
    "    \n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        node['right'] = end_of_split(left+right)\n",
    "        node['left'] = end_of_split(left+right)\n",
    "        return\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = end_of_split(left)\n",
    "        node['right'] = end_of_split(right)\n",
    "        return \n",
    "    \n",
    "    if len(left) <=  min_size:\n",
    "        node['left'] = end_of_split(left)\n",
    "    else:\n",
    "        node['left'] = get_split_g(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    \n",
    "    if len(right) <=  min_size:\n",
    "        node['right'] = end_of_split(right)\n",
    "    else:\n",
    "        node['right'] = get_split_g(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def make_tree(train, max_depth, min_size):\n",
    "    root = get_split_g(train)\n",
    "    split(root, max_depth, min_size,1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, r):\n",
    "    if r[node['idx']] < node['val']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], r)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], r)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    tr = build_tree(train, max_depth, min_size)\n",
    "    lpredictions = list()\n",
    "    for r in test:\n",
    "        pr = predict(tr, r)\n",
    "        lpredictions.append(pr)\n",
    "    \n",
    "    return lpredictions\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct/float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_csv(filename):\n",
    "    data = pd.read_csv(filename, error_bad_lines=False)\n",
    "    return data.values\n",
    "\n",
    "def cross_validation_split(data, num_folds):\n",
    "    data_split = list()\n",
    "    data_cpy = list(data)\n",
    "    fold_sz = int(len(data)/(num_folds))\n",
    "    for i in range(num_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_sz:\n",
    "            idx = randrange(len(data_cpy))\n",
    "            fold.append(data_cpy.pop(idx))\n",
    "        data_split.append(fold)\n",
    "    return data_split       \n",
    "\n",
    "def eval_algorithm(data, n_folds, max_depth, min_size):\n",
    "    folds = cross_validation_split(data, n_folds)\n",
    "    scores = list()\n",
    "    for i in range(len(folds)):\n",
    "        train_set = list(folds)\n",
    "        train_set.pop(i)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in folds[i]:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = decision_tree(train_set, test_set, max_depth, min_size)\n",
    "        actual = [row[-1] for row in folds[i]]\n",
    "        acc = accuracy_metric(actual, predicted)\n",
    "        scores.append(acc)\n",
    "    return scores\n",
    "\n",
    "#correct\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split_g(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandile\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Sandile\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [93.80530973451327, 91.1504424778761, 90.2654867256637, 88.49557522123894, 92.92035398230088]\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')] #drop unnamed column\n",
    "df_xtrain = df.drop(['id','diagnosis'], axis=1)\n",
    "x_train = df_xtrain.values\n",
    "#  Note: Makes M = 1, B=0\n",
    "df['diagnosis'] = np.unique(df['diagnosis'], return_index=True, return_inverse=True)[2]\n",
    "y_train = df['diagnosis'].values\n",
    "y_train.shape\n",
    "dataset = np.hstack((x_train, y_train[:,np.newaxis]))\n",
    "# dataset[:,-1]\n",
    "n_folds = 5\n",
    "max_depth = 7\n",
    "min_size = 5\n",
    "scores = eval_algorithm(dataset, n_folds, max_depth, min_size)\n",
    "\n",
    "print('Scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandile\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Sandile\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [77.00729927007299, 74.45255474452554, 79.56204379562044, 72.26277372262774, 74.81751824817519]\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "dataset1 = l_csv('data_banknote_authentication.csv')\n",
    "dataset1.shape\n",
    "dataset1[:,-1]\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 3\n",
    "scores = eval_algorithm(dataset1, n_folds, max_depth, min_size)\n",
    "print('Scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
